---
title: Unified API
icon: "pyramid"
description: "Access 7 major AI providers through a single, OpenAI SDK-compatible API endpoint"
---

<Info>The Unified API exposes a single, OpenAI-compatible endpoint for accessing 7 major AI providers. This eliminates vendor lock-in and streamlines integration—no code changes required to switch providers.</Info>

## Overview

The Unified API standardizes how your applications interact with leading AI models. Use the familiar OpenAI API format, request structure, and response schema—regardless of the underlying provider. This approach delivers:

<Columns cols={2}>
  <Card title="Vendor Independence" icon="shuffle">
    Swap providers instantly without code changes. Avoid vendor lock-in and maintain flexibility in your AI strategy.
  </Card>
  <Card title="Reduced Development Cost" icon="dollar-sign">
    Integrate once and access many models. Minimize engineering effort and lower ongoing maintenance costs.
  </Card>
  <Card title="Operational Simplicity" icon="activity">
    Centralized management and monitoring for all providers. Streamline operations and simplify troubleshooting.
  </Card>
  <Card title="Strategic Flexibility" icon="compass">
    Leverage each provider’s unique strengths. Easily adapt to new capabilities and optimize for your use cases.
  </Card>
</Columns>

By adopting the Unified API, organizations can focus on building innovative AI applications rather than managing complex integrations, ultimately accelerating their AI transformation journey while maintaining full control over their AI strategy.


## Key Benefits

<AccordionGroup>

  <Accordion title="Vendor Independence & Risk Mitigation" icon="shuffle">
    <ul>
      <li><strong>Eliminate vendor lock-in:</strong> Switch between 7 providers without code changes.</li>
      <li><strong>Business continuity:</strong> Seamless failover during outages.</li>
      <li><strong>Cost optimization:</strong> Dynamically select cost-effective providers.</li>
      <li><strong>Load distribution:</strong> Balance workloads to avoid single points of failure.</li>
    </ul>
  </Accordion>

  <Accordion title="Accelerated Time-to-Market" icon="zap">
    <ul>
      <li><strong>Rapid integration:</strong> Use existing OpenAI SDKs and libraries.</li>
      <li><strong>Unified development:</strong> One API to learn and maintain.</li>
      <li><strong>Reduced development costs:</strong> No need for multiple integration teams.</li>
      <li><strong>Faster experimentation:</strong> Test models/providers with minimal overhead.</li>
    </ul>
  </Accordion>

  <Accordion title="Operational Excellence" icon="activity">
    <ul>
      <li><strong>Simplified infrastructure:</strong> Single endpoint for all AI services.</li>
      <li><strong>Centralized management:</strong> Unified monitoring, logging, and security.</li>
      <li><strong>Compliance ready:</strong> Centralized governance and controls.</li>
      <li><strong>Performance optimization:</strong> Built-in caching and request optimization.</li>
    </ul>
  </Accordion>

  <Accordion title="Strategic Flexibility" icon="compass">
    <ul>
      <li><strong>Future-proof:</strong> Add new providers without app changes.</li>
      <li><strong>Best-of-breed:</strong> Choose optimal models for each use case.</li>
      <li><strong>Negotiation leverage:</strong> Easy switching strengthens your position.</li>
      <li><strong>Innovation agility:</strong> Quickly adopt new AI capabilities.</li>
    </ul>
  </Accordion>

</AccordionGroup>

## Key Capabilities

- **OpenAI SDK Compatibility**: Use official OpenAI client libraries with any provider
- **Consistent API Format**: Same `/v1/chat/completions` endpoint for all providers
- **Automatic Format Translation**: Seamless conversion between OpenAI format and provider-specific formats
- **Full Feature Support**: Streaming, function calling, and multi-modal capabilities where available
- **Zero Code Changes**: Switch providers by changing only configuration headers
- **Enterprise Controls**: Built-in guardrails, compliance, and monitoring across all providers

## How It Works

### Architecture Flow

```
Your Application
      ↓
[OpenAI SDK / REST API]
      ↓
AI Gateway (Unified API)
      ↓
[Automatic Format Translation]
      ↓
Target AI Provider
```

### Request Flow

1. **Client Request**: Your application sends an OpenAI-formatted request
2. **Provider Detection**: Gateway identifies target provider from headers
3. **Format Translation**: Request is automatically converted to provider's native format
4. **Provider Call**: Gateway forwards the transformed request to the AI provider
5. **Response Translation**: Provider's response is converted back to OpenAI format
6. **Unified Response**: Your application receives a standard OpenAI-compatible response

## Supported Providers

| Provider | Description | Key Features |
|----------|-------------|--------------|
| **OpenAI** | Access OpenAI models through the unified endpoint | Full SDK compatibility, GPT-4o, GPT-4o-mini |
| **Anthropic** | Connect to Claude models with built-in guardrails | Constitutional AI, 200K context, safety features |
| **Amazon Bedrock** | Integrate AWS Bedrock models | Enterprise security, HIPAA compliance, AWS integration |
| **Azure OpenAI** | Access Azure-hosted OpenAI models | Microsoft ecosystem, enterprise SLAs |
| **Azure AI Inference** | Leverage Azure AI Inference models | Custom models, Azure ML integration |
| **Google AI** | Integrate Google Gemini models | Multi-modal capabilities, Google ecosystem |
| **Google Vertex AI** | Access Vertex AI models | GCP integration, MLOps capabilities |

## Quick Start

1. **Set Up Gateway**: Deploy the AI Gateway to your infrastructure
2. **Configure Client**: Use OpenAI SDK with provider-specific headers
3. **Make Requests**: Send requests using the unified API format
4. **Switch Providers**: Change providers by updating headers only

## Implementation Examples

All examples use the OpenAI SDK with provider-specific headers to route requests through the AI Gateway.

### Basic Setup Pattern

```python
from openai import OpenAI

client = OpenAI(
    api_key="your-provider-api-key",  # Varies by provider
    base_url="https://your-gateway-url/v1",
    default_headers={
        "x-provider-name": "provider_name",
        "x-altrumai-key": "your-gateway-api-key"
        # Additional provider-specific headers as needed
    }
)
```

### Provider-Specific Examples

#### OpenAI

```python
from openai import OpenAI

client = OpenAI(
    api_key="your-openai-api-key",
    base_url="https://your-gateway-url/v1",
    default_headers={
        "x-provider-name": "openai",
        "x-altrumai-key": "your-gateway-api-key"
    }
)

response = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "user", "content": "What is the capital of France?"}
    ]
)

print(response.choices[0].message.content)
```

#### Anthropic

```python
from openai import OpenAI

client = OpenAI(
    api_key="your-anthropic-api-key",
    base_url="https://your-gateway-url/v1",
    default_headers={
        "x-api-key": "your-api-key",
        "x-provider-name": "anthropic",
        "x-altrumai-key": "your-gateway-api-key"
    }
)

response = client.chat.completions.create(
    model="claude-3-opus-20240229",
    messages=[
        {"role": "user", "content": "Explain quantum computing in simple terms."}
    ]
)

print(response.choices[0].message.content)
```

#### Amazon Bedrock

```python
from openai import OpenAI

client = OpenAI(
    api_key="unused-placeholder",  # Not used, credentials via headers
    base_url="https://your-gateway-url/v1",
    default_headers={
        "x-provider-name": "bedrock",
        "x-bedrock-access-key-id": "your-aws-access-key-id",
        "x-bedrock-secret-access-key": "your-aws-secret-access-key",
        "x-bedrock-region": "your-aws-region",
        "x-bedrock-session-token": "your-aws-session-token",  # Optional
        "x-altrumai-key": "your-gateway-api-key"
    }
)

response = client.chat.completions.create(
    model="anthropic.claude-v2",
    messages=[
        {"role": "user", "content": "Give me a list of 5 creative startup ideas in the AI space."}
    ]
)

print(response.choices[0].message.content)
```

#### Azure OpenAI

```python
from openai import OpenAI

client = OpenAI(
    api_key="unused-placeholder",  # Authentication via headers
    base_url="https://your-gateway-url/v1",
    default_headers={
        "x-provider-name": "azure_openai",
        "x-azure-api-key": "your-azure-api-key",
        "x-azure-resource-name": "your-azure-resource-name",
        "x-azure-deployment-id": "your-azure-deployment-id",
        "x-azure-api-version": "2024-02-15-preview",
        "x-altrumai-key": "your-gateway-api-key"
    }
)

response = client.chat.completions.create(
    model="gpt-35-turbo",
    messages=[
        {"role": "user", "content": "What are the advantages of using managed Kubernetes services?"}
    ]
)

print(response.choices[0].message.content)
```

#### Azure AI Inference

```python
from openai import OpenAI

client = OpenAI(
    api_key="unused-placeholder",  # Authentication via headers
    base_url="https://your-gateway-url/v1",
    default_headers={
        "x-provider-name": "azure_ai_inference",
        "x-altrumai-key": "your-gateway-api-key",
        "x-azure-ai-token": "your-azure-ai-inference-api-key",
        "x-azure-ai-endpoint": "your-azure-ai-inference-endpoint"
    }
)

response = client.chat.completions.create(
    model="phi-2",
    messages=[
        {"role": "user", "content": "Summarize the main benefits of using serverless architectures."}
    ]
)

print(response.choices[0].message.content)
```

#### Google AI

```python
from openai import OpenAI

client = OpenAI(
    api_key="unused-placeholder",  # Authentication via headers
    base_url="https://your-gateway-url/v1",
    default_headers={
        "x-provider-name": "google",
        "x-goog-api-key": "your-google-api-key",
        "x-altrumai-key": "your-gateway-api-key"
    }
)

response = client.chat.completions.create(
    model="gemini-1.5-pro",
    messages=[
        {"role": "user", "content": "How can AI help improve energy efficiency in smart buildings?"}
    ]
)

print(response.choices[0].message.content)
```

#### Google Vertex AI

```python
from openai import OpenAI

client = OpenAI(
    api_key="unused-placeholder",  # Authentication via headers
    base_url="https://your-gateway-url/v1",
    default_headers={
        "x-provider-name": "google_vertex_ai",
        "x-api-key": "your-google-vertex-ai-api-key",
        "x-endpoint-base": "your-google-vertex-ai-endpoint-base",
        "x-project-id": "your-google-vertex-ai-project-id",
        "x-location": "your-google-vertex-ai-location",
        "x-altrumai-key": "your-gateway-api-key"
    }
)

response = client.chat.completions.create(
    model="google.models.text-bison",
    messages=[
        {"role": "user", "content": "List three practical applications of generative AI in education."}
    ]
)

print(response.choices[0].message.content)
```

## Advanced Features

### Streaming Responses

> Streaming works identically across all providers with the same OpenAI SDK interface.

```python
# Streaming works identically across all providers
client = create_ai_client("anthropic", {"api_key": "your-key"})

stream = client.chat.completions.create(
    model="claude-3-5-sonnet-20241022",
    messages=[{"role": "user", "content": "Write a story"}],
    stream=True  # Enable streaming
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end='')
```

### Function Calling

> Function calling is supported across multiple providers with automatic format conversion.

```typescript
// Function calling with automatic format conversion
const response = await client.chat.completions.create({
    model: 'gpt-4o-mini',  // or Claude, Gemini, etc.
    messages: [
        { role: 'user', content: 'What is the weather in London?' }
    ],
    tools: [{
        type: 'function',
        function: {
            name: 'get_weather',
            description: 'Get weather for a location',
            parameters: {
                type: 'object',
                properties: {
                    location: { type: 'string' },
                    unit: { type: 'string', enum: ['celsius', 'fahrenheit'] }
                },
                required: ['location']
            }
        }
    }],
    tool_choice: 'auto'
});
```

### Provider Switching

> Switch between providers seamlessly without changing your application code.

```python
# Demonstration of seamless provider switching
def analyze_text(text: str, provider: str):
    """Same function works with any provider"""
    
    # Provider configurations
    configs = {
        "openai": {
            "model": "gpt-4o-mini",
            "headers": {"Authorization": f"Bearer {os.getenv('OPENAI_KEY')}"}
        },
        "anthropic": {
            "model": "claude-3-5-sonnet-20241022",
            "headers": {"x-api-key": os.getenv('ANTHROPIC_KEY')}
        },
        "google": {
            "model": "gemini-1.5-pro",
            "headers": {"x-goog-api-key": os.getenv('GOOGLE_KEY')}
        }
    }
    
    config = configs[provider]
    config["headers"]["x-provider-name"] = provider
    
    client = OpenAI(
        api_key="dummy",
        base_url="https://gateway.your-domain.com",
        default_headers=config["headers"]
    )
    
    # Identical API call regardless of provider
    return client.chat.completions.create(
        model=config["model"],
        messages=[
            {"role": "system", "content": "You are a text analyst."},
            {"role": "user", "content": f"Analyze this text: {text}"}
        ],
        temperature=0.7
    )

# Switch providers without changing code
result_openai = analyze_text("Sample text", "openai")
result_claude = analyze_text("Sample text", "anthropic")
result_gemini = analyze_text("Sample text", "google")
```

## Implementation Guide

### Step 1: Set Up the Gateway

```bash
# Deploy the AI Gateway
docker run -d \
  -p 3000:3000 \
  -e PORT=3000 \
  --name ai-gateway \
  your-registry/ai-gateway:latest
```

### Step 2: Configure Your Application

```python
# Update your application configuration
AI_GATEWAY_URL = "https://gateway.altrum.ai"

# Replace provider-specific SDKs with OpenAI SDK
# Before: anthropic.Client, googleai.Client, etc.
# After: Just OpenAI client for everything
```

### Step 3: Update API Calls

```python
# Before: Multiple SDK implementations
if provider == "openai":
    openai_client.completions.create(...)
elif provider == "anthropic":
    anthropic_client.messages.create(...)
elif provider == "google":
    google_client.generate_content(...)

# After: Single unified implementation
response = openai_client.chat.completions.create(
    model=model_name,
    messages=messages
)
```

### Step 4: Add Provider Switching Logic

```python
def get_ai_response(prompt: str, preferred_provider: str = None):
    """Intelligent provider selection with fallback"""
    
    providers = ["openai", "anthropic", "google"]
    if preferred_provider:
        providers = [preferred_provider] + [p for p in providers if p != preferred_provider]
    
    for provider in providers:
        try:
            client = create_ai_client(provider, get_credentials(provider))
            return client.chat.completions.create(
                model=get_model_for_provider(provider),
                messages=[{"role": "user", "content": prompt}]
            )
        except Exception as e:
            logger.warning(f"Provider {provider} failed: {e}")
            continue
    
    raise Exception("All providers failed")
```

## Best Practices

### Provider Selection Strategy

```python
class ProviderSelector:
    """Intelligent provider selection based on use case"""
    
    def select_provider(self, use_case: str, requirements: dict):
        if use_case == "long_context":
            return "anthropic"  # Claude handles 200K tokens
        elif use_case == "multimodal":
            return "google"  # Gemini excels at vision tasks
        elif use_case == "function_calling":
            return "openai"  # Most mature function calling
        elif requirements.get("aws_integration"):
            return "bedrock"  # Native AWS integration
        elif requirements.get("azure_integration"):
            return "azure_openai"  # Azure ecosystem
        else:
            return "openai"  # Default fallback
```

### Error Handling

```typescript
async function robustAICall(prompt: string) {
    const providers = ['openai', 'anthropic', 'google'];
    const errors = [];
    
    for (const provider of providers) {
        try {
            const client = createAIClient(provider, getCredentials(provider));
            return await client.chat.completions.create({
                model: getModelForProvider(provider),
                messages: [{ role: 'user', content: prompt }]
            });
        } catch (error) {
            errors.push({ provider, error });
            console.error(`Provider ${provider} failed:`, error);
        }
    }
    
    throw new Error(`All providers failed: ${JSON.stringify(errors)}`);
}
```

### Cost Optimization

```python
def cost_optimized_request(prompt: str, max_cost_usd: float = 0.01):
    """Select provider based on cost constraints"""
    
    # Cost per 1K tokens (approximate)
    provider_costs = {
        "openai": {"gpt-4o-mini": 0.00015},
        "anthropic": {"claude-3-5-haiku": 0.00025},
        "google": {"gemini-1.5-flash": 0.00010}
    }
    
    # Select cheapest provider within budget
    for provider, models in sorted(provider_costs.items(), 
                                   key=lambda x: min(x[1].values())):
        model = min(models, key=models.get)
        if models[model] * (len(prompt) / 1000) <= max_cost_usd:
            return make_request(provider, model, prompt)
```

### Monitoring and Observability

```python
import time
import logging

def monitored_ai_call(client, **kwargs):
    """Add monitoring to AI calls"""
    
    start_time = time.time()
    provider = client.default_headers.get('x-provider-name')
    
    try:
        response = client.chat.completions.create(**kwargs)
        
        # Log success metrics
        logging.info({
            'provider': provider,
            'model': kwargs.get('model'),
            'latency': time.time() - start_time,
            'tokens': response.usage.total_tokens,
            'status': 'success'
        })
        
        return response
        
    except Exception as e:
        # Log failure metrics
        logging.error({
            'provider': provider,
            'model': kwargs.get('model'),
            'latency': time.time() - start_time,
            'error': str(e),
            'status': 'failed'
        })
        raise
```

## Migration Guide

### From Native SDKs to Unified API

#### Before (Multiple SDKs)

```python
# OpenAI
import openai
openai_response = openai.ChatCompletion.create(...)

# Anthropic
import anthropic
claude_response = anthropic.Anthropic().messages.create(...)

# Google
import google.generativeai as genai
gemini_response = genai.GenerativeModel().generate_content(...)
```

#### After (Unified API)

```python
from openai import OpenAI

# One client, multiple providers
def get_response(provider: str):
    client = OpenAI(
        base_url="https://gateway.your-domain.com",
        default_headers={"x-provider-name": provider, **get_auth_headers(provider)}
    )
    return client.chat.completions.create(...)
```

## Supported AI Providers

### OpenAI
**Models**: GPT-4o, GPT-4o-mini, GPT-4-Turbo, GPT-3.5-Turbo  
**Strengths**: Industry-leading language models, extensive function calling support, robust ecosystem  
**Use Cases**: General-purpose AI, complex reasoning, code generation, creative writing

### Anthropic Claude
**Models**: Claude 3.5 Sonnet, Claude 3.5 Haiku, Claude 3 Opus  
**Strengths**: Superior context handling (200K tokens), constitutional AI for safety, nuanced responses  
**Use Cases**: Long-form content analysis, research, technical documentation, ethical AI applications

### Amazon Bedrock
**Models**: Multiple providers including Anthropic Claude, Meta Llama, Amazon Titan  
**Strengths**: AWS integration, enterprise security, multi-model access, HIPAA compliance  
**Use Cases**: Enterprise deployments, regulated industries, AWS-native applications

### Azure OpenAI
**Models**: GPT-4, GPT-3.5, DALL-E, Embeddings  
**Strengths**: Microsoft ecosystem integration, enterprise SLAs, regional deployments  
**Use Cases**: Microsoft 365 integration, enterprise applications, global deployments

### Azure AI Inference
**Models**: Various models from Azure AI Studio catalog  
**Strengths**: Model variety, Azure ML integration, custom model deployment  
**Use Cases**: Specialized AI models, custom fine-tuned models, Azure ML workflows

### Google AI (Gemini)
**Models**: Gemini 1.5 Pro, Gemini 1.5 Flash, Gemini 1.0 Pro  
**Strengths**: Multi-modal capabilities, large context windows, Google ecosystem  
**Use Cases**: Multi-modal applications, Google Workspace integration, consumer applications

### Google Vertex AI
**Models**: Gemini models, PaLM, custom models  
**Strengths**: GCP integration, MLOps capabilities, enterprise features  
**Use Cases**: GCP-native applications, ML pipelines, enterprise AI deployments

---

## Key Benefits Summary

- **Unified Interface**: Single API endpoint for all providers
- **SDK Compatibility**: Works with existing OpenAI SDK code
- **Enterprise Controls**: Built-in guardrails, compliance, and monitoring
- **Provider Flexibility**: Easy switching between AI providers
- **Consistent Authentication**: Standardized header-based authentication
- **Cost Optimization**: Centralized usage tracking and optimization