---
title: Unified API
icon: "pyramid"
description: "Access 7 major AI providers through a single, OpenAI SDK-compatible API endpoint"
---

<Info>The Unified API exposes a single, OpenAI-compatible endpoint for accessing 7 major AI providers. This eliminates vendor lock-in and streamlines integration—no code changes required to switch providers.</Info>

## Overview

The Unified API standardizes how your applications interact with leading AI models. Use the familiar OpenAI API format, request structure, and response schema—regardless of the underlying provider. This approach delivers:

<Columns cols={2}>
  <Card title="Vendor Independence" icon="shuffle">
    Swap providers instantly without code changes. Avoid vendor lock-in and maintain flexibility in your AI strategy.
  </Card>
  <Card title="Reduced Development Cost" icon="dollar-sign">
    Integrate once and access many models. Minimize engineering effort and lower ongoing maintenance costs.
  </Card>
  <Card title="Operational Simplicity" icon="activity">
    Centralized management and monitoring for all providers. Streamline operations and simplify troubleshooting.
  </Card>
  <Card title="Strategic Flexibility" icon="compass">
    Leverage each provider’s unique strengths. Easily adapt to new capabilities and optimize for your use cases.
  </Card>
</Columns>

By adopting the Unified API, organizations can focus on building innovative AI applications rather than managing complex integrations, ultimately accelerating their AI transformation journey while maintaining full control over their AI strategy.

### How it Works

<img height="200" src="/images/unified-api-request-diagram.png" />

## Supported Providers

| Provider | Description |
|----------|-------------|
| **OpenAI** | Access OpenAI models through the unified endpoint |
| **Anthropic** | Connect to Claude models with built-in guardrails |
| **Amazon Bedrock** | Integrate AWS Bedrock models |
| **Azure OpenAI** | Access Azure-hosted OpenAI models |
| **Azure AI Inference** | Leverage Azure AI Inference models |
| **Google AI** | Integrate Google Gemini models |
| **Google Vertex AI** | Access Vertex AI models |

## Implementation Guide

<Steps>
  <Step title="Set Up Gateway">
    Deploy the AI Gateway to your infrastructure. This provides a unified endpoint for all supported AI providers.
  </Step>
  <Step title="Configure Client">
    Use the OpenAI SDK and set provider-specific headers (such as <code>x-provider-name</code>) to route requests through the gateway.
  </Step>
  <Step title="Make Requests">
    Send requests using the standard OpenAI API format. The gateway will automatically translate and forward them to the selected provider.
  </Step>
  <Step title="Switch Providers">
    Change providers at any time by updating the <code>x-provider-name</code> header—no code changes required.
  </Step>
</Steps>

## Implementation Examples for Each Provider

<Note>All examples use the OpenAI SDK with provider-specific headers to route requests through the AI Gateway.</Note>

#### OpenAI
Access OpenAI models using the unified API endpoint and standard OpenAI SDK.

<CodeGroup dropdown>

```python openai_example.py
from openai import OpenAI

client = OpenAI(
    api_key="your-openai-api-key",
    base_url="https://your-gateway-url/v1",
    default_headers={
        "x-provider-name": "openai",
        "x-altrumai-key": "your-gateway-api-key"
    }
)

response = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "user", "content": "What is the capital of France?"}
    ]
)

print(response.choices[0].message.content)
```

```bash openai_curl.sh
curl -X POST "https://your-gateway-url/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your-openai-api-key" \
  -H "x-provider-name: openai" \
  -H "x-altrumai-key: your-gateway-api-key" \
  -d '{
    "model": "gpt-4",
    "messages": [
      {
        "role": "user",
        "content": "What is the capital of France?"
      }
    ]
  }'
```

</CodeGroup>

#### Anthropic
Access Anthropic Claude models using the unified API endpoint and standard OpenAI SDK.

<CodeGroup dropdown>

```python anthropic_example.py
from openai import OpenAI

client = OpenAI(
    api_key="your-anthropic-api-key",
    base_url="https://gateway.altrum.ai/v1",
    default_headers={
        "x-api-key": "your-api-key",
        "x-provider-name": "anthropic",
        "x-altrumai-key": "your-altrumai-api-key"
    }
)

response = client.chat.completions.create(
    model="claude-3-opus-20240229",
    messages=[
        {"role": "user", "content": "Explain quantum computing in simple terms."}
    ]
)

print(response.choices[0].message.content)
```

```bash anthropic_curl.sh
curl -X POST "https://gateway.altrum.ai/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your-anthropic-api-key" \
  -H "x-api-key: your-api-key" \
  -H "x-provider-name: anthropic" \
  -H "x-altrumai-key: your-altrumai-api-key" \
  -d '{
    "model": "claude-3-opus-20240229",
    "messages": [
      {
        "role": "user",
        "content": "Explain quantum computing in simple terms."
      }
    ]
  }'
```

</CodeGroup>

#### Amazon Bedrock
Access Amazon Bedrock models using the unified API endpoint and standard OpenAI SDK.

<CodeGroup dropdown>

```python bedrock_example.py
from openai import OpenAI

client = OpenAI(
    api_key="unused-placeholder",  # Not used, credentials via headers
    base_url="https://your-gateway-url/v1",
    default_headers={
        "x-provider-name": "bedrock",
        "x-bedrock-access-key-id": "your-aws-access-key-id",
        "x-bedrock-secret-access-key": "your-aws-secret-access-key",
        "x-bedrock-region": "your-aws-region",
        "x-bedrock-session-token": "your-aws-session-token",  # Optional
        "x-altrumai-key": "your-gateway-api-key"
    }
)

response = client.chat.completions.create(
    model="anthropic.claude-v2",
    messages=[
        {"role": "user", "content": "Give me a list of 5 creative startup ideas in the AI space."}
    ]
)

print(response.choices[0].message.content)
```

```bash bedrock_curl.sh
curl -X POST "https://your-gateway-url/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "x-provider-name: bedrock" \
  -H "x-bedrock-access-key-id: your-aws-access-key-id" \
  -H "x-bedrock-secret-access-key: your-aws-secret-access-key" \
  -H "x-bedrock-region: your-aws-region" \
  -H "x-bedrock-session-token: your-aws-session-token" \
  -H "x-altrumai-key: your-gateway-api-key" \
  -d '{
    "model": "anthropic.claude-v2",
    "messages": [
      {
        "role": "user",
        "content": "Give me a list of 5 creative startup ideas in the AI space."
      }
    ]
  }'
```

</CodeGroup>

#### Azure OpenAI
Access Azure OpenAI models through the unified API using standard OpenAI SDK and gateway headers.

<CodeGroup dropdown>

```python azure_openai_example.py
from openai import OpenAI

client = OpenAI(
    api_key="unused-placeholder",  # Authentication via headers
    base_url="https://your-gateway-url/v1",
    default_headers={
        "x-provider-name": "azure_openai",
        "x-azure-api-key": "your-azure-api-key",
        "x-azure-resource-name": "your-azure-resource-name",
        "x-azure-deployment-id": "your-azure-deployment-id",
        "x-azure-api-version": "2024-02-15-preview",
        "x-altrumai-key": "your-gateway-api-key"
    }
)

response = client.chat.completions.create(
    model="gpt-35-turbo",
    messages=[
        {"role": "user", "content": "What are the advantages of using managed Kubernetes services?"}
    ]
)

print(response.choices[0].message.content)
```

```bash azure_openai_curl.sh
curl -X POST "https://your-gateway-url/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "x-provider-name: azure_openai" \
  -H "x-azure-api-key: your-azure-api-key" \
  -H "x-azure-resource-name: your-azure-resource-name" \
  -H "x-azure-deployment-id: your-azure-deployment-id" \
  -H "x-azure-api-version: 2024-02-15-preview" \
  -H "x-altrumai-key: your-gateway-api-key" \
  -d '{
    "model": "gpt-35-turbo",
    "messages": [
      {
        "role": "user",
        "content": "What are the advantages of using managed Kubernetes services?"
      }
    ]
  }'
```

</CodeGroup>

#### Azure AI Inference
Access Azure AI Inference models via the unified API using standard OpenAI SDK and gateway headers.

<CodeGroup dropdown>

```python azure_ai_inference_example.py
from openai import OpenAI

client = OpenAI(
    api_key="unused-placeholder",  # Authentication via headers
    base_url="https://your-gateway-url/v1",
    default_headers={
        "x-provider-name": "azure_ai_inference",
        "x-altrumai-key": "your-gateway-api-key",
        "x-azure-ai-token": "your-azure-ai-inference-api-key",
        "x-azure-ai-endpoint": "your-azure-ai-inference-endpoint"
    }
)

response = client.chat.completions.create(
    model="phi-2",
    messages=[
        {"role": "user", "content": "Summarize the main benefits of using serverless architectures."}
    ]
)

print(response.choices[0].message.content)
```

```bash azure_ai_inference_curl.sh
curl -X POST "https://your-gateway-url/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "x-provider-name: azure_ai_inference" \
  -H "x-altrumai-key: your-gateway-api-key" \
  -H "x-azure-ai-token: your-azure-ai-inference-api-key" \
  -H "x-azure-ai-endpoint: your-azure-ai-inference-endpoint" \
  -d '{
    "model": "phi-2",
    "messages": [
      {
        "role": "user",
        "content": "Summarize the main benefits of using serverless architectures."
      }
    ]
  }'
```

</CodeGroup>

#### Google AI
Access Google Gemini models via the unified API using standard OpenAI SDK and gateway headers.

<CodeGroup dropdown>

```python google_ai_example.py
from openai import OpenAI

client = OpenAI(
    api_key="unused-placeholder",  # Authentication via headers
    base_url="https://your-gateway-url/v1",
    default_headers={
        "x-provider-name": "google",
        "x-goog-api-key": "your-google-api-key",
        "x-altrumai-key": "your-gateway-api-key"
    }
)

response = client.chat.completions.create(
    model="gemini-1.5-pro",
    messages=[
        {"role": "user", "content": "How can AI help improve energy efficiency in smart buildings?"}
    ]
)

print(response.choices[0].message.content)
```

```bash google_ai_curl.sh
curl -X POST "https://your-gateway-url/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "x-provider-name: google" \
  -H "x-goog-api-key: your-google-api-key" \
  -H "x-altrumai-key: your-gateway-api-key" \
  -d '{
    "model": "gemini-1.5-pro",
    "messages": [
      {
        "role": "user",
        "content": "How can AI help improve energy efficiency in smart buildings?"
      }
    ]
  }'
```

</CodeGroup>

#### Google Vertex AI
Access Google Vertex AI models through the unified API using standard OpenAI SDK and gateway headers.

<CodeGroup dropdown>

```python google_vertex_ai_example.py
from openai import OpenAI

client = OpenAI(
    api_key="unused-placeholder",  # Authentication via headers
    base_url="https://your-gateway-url/v1",
    default_headers={
        "x-provider-name": "google_vertex_ai",
        "x-api-key": "your-google-vertex-ai-api-key",
        "x-endpoint-base": "your-google-vertex-ai-endpoint-base",
        "x-project-id": "your-google-vertex-ai-project-id",
        "x-location": "your-google-vertex-ai-location",
        "x-altrumai-key": "your-gateway-api-key"
    }
)

response = client.chat.completions.create(
    model="google.models.text-bison",
    messages=[
        {"role": "user", "content": "List three practical applications of generative AI in education."}
    ]
)

print(response.choices[0].message.content)
```

```bash google_vertex_ai_curl.sh
curl -X POST "https://your-gateway-url/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "x-provider-name: google_vertex_ai" \
  -H "x-api-key: your-google-vertex-ai-api-key" \
  -H "x-endpoint-base: your-google-vertex-ai-endpoint-base" \
  -H "x-project-id: your-google-vertex-ai-project-id" \
  -H "x-location: your-google-vertex-ai-location" \
  -H "x-altrumai-key: your-gateway-api-key" \
  -d '{
    "model": "google.models.text-bison",
    "messages": [
      {
        "role": "user",
        "content": "List three practical applications of generative AI in education."
      }
    ]
  }'
```

</CodeGroup>

## Best Practices

### Provider Selection Strategy

```python
class ProviderSelector:
    """Intelligent provider selection based on use case"""
    
    def select_provider(self, use_case: str, requirements: dict):
        if use_case == "long_context":
            return "anthropic"  # Claude handles 200K tokens
        elif use_case == "multimodal":
            return "google"  # Gemini excels at vision tasks
        elif use_case == "function_calling":
            return "openai"  # Most mature function calling
        elif requirements.get("aws_integration"):
            return "bedrock"  # Native AWS integration
        elif requirements.get("azure_integration"):
            return "azure_openai"  # Azure ecosystem
        else:
            return "openai"  # Default fallback
```

### Error Handling

```typescript
async function robustAICall(prompt: string) {
    const providers = ['openai', 'anthropic', 'google'];
    const errors = [];
    
    for (const provider of providers) {
        try {
            const client = createAIClient(provider, getCredentials(provider));
            return await client.chat.completions.create({
                model: getModelForProvider(provider),
                messages: [{ role: 'user', content: prompt }]
            });
        } catch (error) {
            errors.push({ provider, error });
            console.error(`Provider ${provider} failed:`, error);
        }
    }
    
    throw new Error(`All providers failed: ${JSON.stringify(errors)}`);
}
```

### Cost Optimization

```python
def cost_optimized_request(prompt: str, max_cost_usd: float = 0.01):
    """Select provider based on cost constraints"""
    
    # Cost per 1K tokens (approximate)
    provider_costs = {
        "openai": {"gpt-4o-mini": 0.00015},
        "anthropic": {"claude-3-5-haiku": 0.00025},
        "google": {"gemini-1.5-flash": 0.00010}
    }
    
    # Select cheapest provider within budget
    for provider, models in sorted(provider_costs.items(), 
                                   key=lambda x: min(x[1].values())):
        model = min(models, key=models.get)
        if models[model] * (len(prompt) / 1000) <= max_cost_usd:
            return make_request(provider, model, prompt)
```

### Monitoring and Observability

```python
import time
import logging

def monitored_ai_call(client, **kwargs):
    """Add monitoring to AI calls"""
    
    start_time = time.time()
    provider = client.default_headers.get('x-provider-name')
    
    try:
        response = client.chat.completions.create(**kwargs)
        
        # Log success metrics
        logging.info({
            'provider': provider,
            'model': kwargs.get('model'),
            'latency': time.time() - start_time,
            'tokens': response.usage.total_tokens,
            'status': 'success'
        })
        
        return response
        
    except Exception as e:
        # Log failure metrics
        logging.error({
            'provider': provider,
            'model': kwargs.get('model'),
            'latency': time.time() - start_time,
            'error': str(e),
            'status': 'failed'
        })
        raise
```

## Migration Guide

### From Native SDKs to Unified API

#### Before (Multiple SDKs)

```python
# OpenAI
import openai
openai_response = openai.ChatCompletion.create(...)

# Anthropic
import anthropic
claude_response = anthropic.Anthropic().messages.create(...)

# Google
import google.generativeai as genai
gemini_response = genai.GenerativeModel().generate_content(...)
```

#### After (Unified API)

```python
from openai import OpenAI

# One client, multiple providers
def get_response(provider: str):
    client = OpenAI(
        base_url="https://gateway.your-domain.com",
        default_headers={"x-provider-name": provider, **get_auth_headers(provider)}
    )
    return client.chat.completions.create(...)
```