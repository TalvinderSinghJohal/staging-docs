---
title: Unified API
description: "Access 7 major AI providers through a single, OpenAI SDK compatible API endpoint"
---

<Info>The Unified API exposes a single, OpenAI compatible endpoint for accessing 7 major AI providers. This eliminates vendor lockin and streamlines integration no code changes required to switch providers.</Info>

## Overview

The Unified API standardises how your applications interact with leading AI models. Use the familiar OpenAI API format, request structure, and response schema regardless of the underlying provider. This approach delivers:

<Columns cols={2}>
  <Card title="Vendor Independence" icon="shuffle">
    Swap providers instantly without code changes. Avoid vendor lock in and maintain flexibility in your AI strategy.
  </Card>
  <Card title="Reduced Development Cost" icon="dollar-sign">
    Integrate once and access many models. Minimise engineering effort and lower ongoing maintenance costs.
  </Card>
  <Card title="Operational Simplicity" icon="activity">
    Centralised management and monitoring for all providers. Streamline operations and simplify troubleshooting.
  </Card>
  <Card title="Strategic Flexibility" icon="compass">
    Leverage each providerâ€™s unique strengths. Easily adapt to new capabilities and optimize for your use cases.
  </Card>
</Columns>

By adopting the Unified API, organisations can focus on building innovative AI applications rather than managing complex integrations, ultimately accelerating their AI transformation journey while maintaining full control over their AI strategy.

### How it Works

<img height="200" src="/images/unified-api-diagram.png" />

## Supported Providers

| Provider | Description |
|----------|-------------|
| **OpenAI** | Access OpenAI models through the unified endpoint |
| **Anthropic** | Connect to Claude models with built in guardrails |
| **Amazon Bedrock** | Integrate AWS Bedrock models |
| **Azure OpenAI** | Access Azure hosted OpenAI models |
| **Azure AI Inference** | Leverage Azure AI Inference models |
| **Google AI** | Integrate Google Gemini models |
| **Google Vertex AI** | Access Vertex AI models |

## Implementation Guide

<Steps>
  <Step title="Set Up Gateway">
    Deploy the AI Gateway to your infrastructure. This provides a unified endpoint for all supported AI providers.
  </Step>
  <Step title="Configure Client">
    Use the OpenAI SDK and set provider specific headers (such as <code>x-provider-name</code>) to route requests through the gateway.
  </Step>
  <Step title="Make Requests">
    Send requests using the standard OpenAI API format. The gateway will automatically translate and forward them to the selected provider.
  </Step>
  <Step title="Switch Providers">
    Change providers at any time by updating the <code>x-provider-name</code> header no code changes required.
  </Step>
</Steps>

## Implementation Examples for Each Provider

<Note>All examples use the OpenAI SDK with provider specific headers to route requests through the AI Gateway.</Note>

#### OpenAI
Access OpenAI models using the unified API endpoint and standard OpenAI SDK.

<CodeGroup dropdown>

```python openai_example.py
from openai import OpenAI

client = OpenAI(
    api_key="your-openai-api-key",
    base_url="https://your-gateway-url/v1",
    default_headers={
        "x-provider-name": "openai",
        "x-altrumai-key": "your-gateway-api-key"
    }
)

response = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "user", "content": "What is the capital of France?"}
    ]
)

print(response.choices[0].message.content)
```

```bash openai_curl.sh
curl -X POST "https://your-gateway-url/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your-openai-api-key" \
  -H "x-provider-name: openai" \
  -H "x-altrumai-key: your-gateway-api-key" \
  -d '{
    "model": "gpt-4",
    "messages": [
      {
        "role": "user",
        "content": "What is the capital of France?"
      }
    ]
  }'
```

</CodeGroup>

#### Anthropic
Access Anthropic Claude models using the unified API endpoint and standard OpenAI SDK.

<CodeGroup dropdown>

```python anthropic_example.py
from openai import OpenAI

client = OpenAI(
    api_key="your-anthropic-api-key",
    base_url="https://gateway.altrum.ai/v1",
    default_headers={
        "x-api-key": "your-api-key",
        "x-provider-name": "anthropic",
        "x-altrumai-key": "your-altrumai-api-key"
    }
)

response = client.chat.completions.create(
    model="claude-3-opus-20240229",
    messages=[
        {"role": "user", "content": "Explain quantum computing in simple terms."}
    ]
)

print(response.choices[0].message.content)
```

```bash anthropic_curl.sh
curl -X POST "https://gateway.altrum.ai/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your-anthropic-api-key" \
  -H "x-api-key: your-api-key" \
  -H "x-provider-name: anthropic" \
  -H "x-altrumai-key: your-altrumai-api-key" \
  -d '{
    "model": "claude-3-opus-20240229",
    "messages": [
      {
        "role": "user",
        "content": "Explain quantum computing in simple terms."
      }
    ]
  }'
```

</CodeGroup>

#### Amazon Bedrock
Access Amazon Bedrock models using the unified API endpoint and standard OpenAI SDK.

<CodeGroup dropdown>

```python bedrock_example.py
from openai import OpenAI

client = OpenAI(
    api_key="unused-placeholder",  # Not used, credentials via headers
    base_url="https://your-gateway-url/v1",
    default_headers={
        "x-provider-name": "bedrock",
        "x-bedrock-access-key-id": "your-aws-access-key-id",
        "x-bedrock-secret-access-key": "your-aws-secret-access-key",
        "x-bedrock-region": "your-aws-region",
        "x-bedrock-session-token": "your-aws-session-token",  # Optional
        "x-altrumai-key": "your-gateway-api-key"
    }
)

response = client.chat.completions.create(
    model="anthropic.claude-v2",
    messages=[
        {"role": "user", "content": "Give me a list of 5 creative startup ideas in the AI space."}
    ]
)

print(response.choices[0].message.content)
```

```bash bedrock_curl.sh
curl -X POST "https://your-gateway-url/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "x-provider-name: bedrock" \
  -H "x-bedrock-access-key-id: your-aws-access-key-id" \
  -H "x-bedrock-secret-access-key: your-aws-secret-access-key" \
  -H "x-bedrock-region: your-aws-region" \
  -H "x-bedrock-session-token: your-aws-session-token" \
  -H "x-altrumai-key: your-gateway-api-key" \
  -d '{
    "model": "anthropic.claude-v2",
    "messages": [
      {
        "role": "user",
        "content": "Give me a list of 5 creative startup ideas in the AI space."
      }
    ]
  }'
```

</CodeGroup>

#### Azure OpenAI
Access Azure OpenAI models through the unified API using standard OpenAI SDK and gateway headers.

<CodeGroup dropdown>

```python azure_openai_example.py
from openai import OpenAI

client = OpenAI(
    api_key="unused-placeholder",  # Authentication via headers
    base_url="https://your-gateway-url/v1",
    default_headers={
        "x-provider-name": "azure_openai",
        "x-azure-api-key": "your-azure-api-key",
        "x-azure-resource-name": "your-azure-resource-name",
        "x-azure-deployment-id": "your-azure-deployment-id",
        "x-azure-api-version": "2024-02-15-preview",
        "x-altrumai-key": "your-gateway-api-key"
    }
)

response = client.chat.completions.create(
    model="gpt-35-turbo",
    messages=[
        {"role": "user", "content": "What are the advantages of using managed Kubernetes services?"}
    ]
)

print(response.choices[0].message.content)
```

```bash azure_openai_curl.sh
curl -X POST "https://your-gateway-url/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "x-provider-name: azure_openai" \
  -H "x-azure-api-key: your-azure-api-key" \
  -H "x-azure-resource-name: your-azure-resource-name" \
  -H "x-azure-deployment-id: your-azure-deployment-id" \
  -H "x-azure-api-version: 2024-02-15-preview" \
  -H "x-altrumai-key: your-gateway-api-key" \
  -d '{
    "model": "gpt-35-turbo",
    "messages": [
      {
        "role": "user",
        "content": "What are the advantages of using managed Kubernetes services?"
      }
    ]
  }'
```

</CodeGroup>

#### Azure AI Inference
Access Azure AI Inference models via the unified API using standard OpenAI SDK and gateway headers.

<CodeGroup dropdown>

```python azure_ai_inference_example.py
from openai import OpenAI

client = OpenAI(
    api_key="unused-placeholder",  # Authentication via headers
    base_url="https://your-gateway-url/v1",
    default_headers={
        "x-provider-name": "azure_ai_inference",
        "x-altrumai-key": "your-gateway-api-key",
        "x-azure-ai-token": "your-azure-ai-inference-api-key",
        "x-azure-ai-endpoint": "your-azure-ai-inference-endpoint"
    }
)

response = client.chat.completions.create(
    model="phi-2",
    messages=[
        {"role": "user", "content": "Summarise the main benefits of using serverless architectures."}
    ]
)

print(response.choices[0].message.content)
```

```bash azure_ai_inference_curl.sh
curl -X POST "https://your-gateway-url/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "x-provider-name: azure_ai_inference" \
  -H "x-altrumai-key: your-gateway-api-key" \
  -H "x-azure-ai-token: your-azure-ai-inference-api-key" \
  -H "x-azure-ai-endpoint: your-azure-ai-inference-endpoint" \
  -d '{
    "model": "phi-2",
    "messages": [
      {
        "role": "user",
        "content": "Summarise the main benefits of using serverless architectures."
      }
    ]
  }'
```

</CodeGroup>

#### Google AI
Access Google Gemini models via the unified API using standard OpenAI SDK and gateway headers.

<CodeGroup dropdown>

```python google_ai_example.py
from openai import OpenAI

client = OpenAI(
    api_key="unused-placeholder",  # Authentication via headers
    base_url="https://your-gateway-url/v1",
    default_headers={
        "x-provider-name": "google",
        "x-goog-api-key": "your-google-api-key",
        "x-altrumai-key": "your-gateway-api-key"
    }
)

response = client.chat.completions.create(
    model="gemini-1.5-pro",
    messages=[
        {"role": "user", "content": "How can AI help improve energy efficiency in smart buildings?"}
    ]
)

print(response.choices[0].message.content)
```

```bash google_ai_curl.sh
curl -X POST "https://your-gateway-url/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "x-provider-name: google" \
  -H "x-goog-api-key: your-google-api-key" \
  -H "x-altrumai-key: your-gateway-api-key" \
  -d '{
    "model": "gemini-1.5-pro",
    "messages": [
      {
        "role": "user",
        "content": "How can AI help improve energy efficiency in smart buildings?"
      }
    ]
  }'
```

</CodeGroup>

#### Google Vertex AI
Access Google Vertex AI models through the unified API using standard OpenAI SDK and gateway headers.

<CodeGroup dropdown>

```python google_vertex_ai_example.py
from openai import OpenAI

client = OpenAI(
    api_key="unused-placeholder",  # Authentication via headers
    base_url="https://your-gateway-url/v1",
    default_headers={
        "x-provider-name": "google_vertex_ai",
        "x-api-key": "your-google-vertex-ai-api-key",
        "x-endpoint-base": "your-google-vertex-ai-endpoint-base",
        "x-project-id": "your-google-vertex-ai-project-id",
        "x-location": "your-google-vertex-ai-location",
        "x-altrumai-key": "your-gateway-api-key"
    }
)

response = client.chat.completions.create(
    model="google.models.text-bison",
    messages=[
        {"role": "user", "content": "List three practical applications of generative AI in education."}
    ]
)

print(response.choices[0].message.content)
```

```bash google_vertex_ai_curl.sh
curl -X POST "https://your-gateway-url/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "x-provider-name: google_vertex_ai" \
  -H "x-api-key: your-google-vertex-ai-api-key" \
  -H "x-endpoint-base: your-google-vertex-ai-endpoint-base" \
  -H "x-project-id: your-google-vertex-ai-project-id" \
  -H "x-location: your-google-vertex-ai-location" \
  -H "x-altrumai-key: your-gateway-api-key" \
  -d '{
    "model": "google.models.text-bison",
    "messages": [
      {
        "role": "user",
        "content": "List three practical applications of generative AI in education."
      }
    ]
  }'
```
</CodeGroup>

## Migration Guide

### From Native SDKs to Unified API

#### Before (Multiple SDKs)

```python
# OpenAI
import openai
openai_response = openai.ChatCompletion.create(...)

# Anthropic
import anthropic
claude_response = anthropic.Anthropic().messages.create(...)

# Google
import google.generativeai as genai
gemini_response = genai.GenerativeModel().generate_content(...)
```

#### After (Unified API)

```python
from openai import OpenAI

# One client, multiple providers
def get_response(provider: str):
    client = OpenAI(
        base_url="https://gateway.your-domain.com",
        default_headers={"x-provider-name": provider, **get_auth_headers(provider)}
    )
    return client.chat.completions.create(...)
```