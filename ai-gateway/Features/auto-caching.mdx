---
title: Auto Caching
description: The AI Gateway's Auto Caching feature delivers intelligent response caching for AI model interactions, automatically storing and serving frequently requested completions to dramatically reduce latency, optimise costs, and improve system performance. Organisations can achieve up to 95% latency reduction for repeated queries while maintaining response freshness and accuracy.
---

## Overview

Auto Caching represents a critical performance optimisation feature that stores AI model responses for reuse, eliminating redundant API calls to upstream providers. This caching system operates transparently across all 7 supported AI providers, ensuring consistent performance improvements regardless of the underlying model or provider.

<Columns cols={2}>
  <Card title="Automatic Response Caching">
    Zero-configuration caching for all non-streaming responses.
  </Card>
  <Card title="Automatic Cache Key Generation">
    Intelligent cache key generation based on request content.
  </Card>
  <Card title="Configurable TTL">
    Flexible time-to-live settings with 60 second default.
  </Card>
  <Card title="Provider Agnostic">
    Works seamlessly across all 7 AI providers.
  </Card>
  <Card title="Performance Metrics">
    Built-in cache hit/miss tracking and latency measurements.
  </Card>
  <Card title="Memory-Efficient Storage">
    Optimised in-memory cache with automatic expiration.
  </Card>
</Columns>

### What Gets Cached

- **Successful Responses Only**: Only 2xx responses are cached
- **Non-Streaming Content**: Standard JSON responses (streaming excluded)
- **Complete Request Context**: Cache keys include full request body and path
- **Provider-Specific Metadata**: Model, provider, and latency information preserved


## How Auto Caching Works

### Cache Key Generation

The caching system generates unique cache keys by combining the request path with the validated request body, ensuring that identical requests receive cached responses while maintaining isolation between different queries.

### Cache Lifecycle Management

#### Write Path
1. **Request Reception**: Incoming request processed by middleware stack
2. **Cache Lookup**: Check for existing valid cache entry
3. **Cache Miss**: Forward request to AI provider
4. **Response Storage**: Store successful responses with TTL
5. **Client Return**: Send response to client

#### Read Path
1. **Request Reception**: Incoming request enters pipeline
2. **Key Generation**: Create cache key from validated request
3. **Cache Hit**: Retrieve stored response if valid
4. **Instant Return**: Serve cached response with minimal latency
5. **Metrics Update**: Record cache hit and time saved

#### Expiration Handling
- **TTL Based Expiration**: Entries expire after configured duration
- **Lazy Cleanup**: Expired entries removed on next access attempt
- **Memory Protection**: Prevents unbounded cache growth
- **Graceful Degradation**: Expired entries trigger fresh requests

## Business Benefits

### Cost Optimisation

#### Direct Cost Savings
- **70-95% Reduction in API Calls**: Eliminate redundant requests to AI providers
- **Token Usage Optimisation**: Reuse responses without consuming additional tokens
- **Bandwidth Savings**: Reduce network transfer costs
- **Provider Cost Reduction**: Lower monthly bills from AI service providers

#### Example Cost Impact
```
Without Caching:
- 10,000 identical requests/day
- $0.02 per request (GPT-4)
- Daily cost: $200
- Monthly cost: $6,000

With 80% Cache Hit Rate:
- 2,000 actual API calls/day
- Daily cost: $40
- Monthly cost: $1,200
- Savings: $4,800/month (80% reduction)
```

#### Indirect Cost Benefits
- **Reduced Infrastructure Needs**: Lower compute requirements
- **Decreased Operational Overhead**: Fewer rate limit issues
- **Improved Resource Utilisation**: Better throughput per dollar spent

### Performance Enhancement

#### Latency Reduction
- **95% Faster Response Times**: Cache hits return in less than 5ms vs 500-2000ms
- **Consistent Performance**: Eliminate provider variability
- **Predictable SLAs**: Meet strict latency requirements
- **Enhanced User Experience**: Near-instant responses for common queries

#### Performance Metrics
```
Typical Latency Comparison:
- Provider API Call: 500-2000ms
- Cache Hit: 1-5ms
- Performance Gain: 100-2000x faster
- Time Saved per Hit: 495-1995ms
```