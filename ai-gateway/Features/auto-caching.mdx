---
title: Auto Caching for AI Gateway
description: The AI Gateway's Auto Caching feature delivers intelligent response caching for AI model interactions, automatically storing and serving frequently requested completions to dramatically reduce latency, optimise costs, and improve system performance. Organisations can achieve up to 95% latency reduction for repeated queries while maintaining response freshness and accuracy.
---

## Overview

Auto Caching represents a critical performance optimisation feature that stores AI model responses for reuse, eliminating redundant API calls to upstream providers. This caching system operates transparently across all 7 supported AI providers, ensuring consistent performance improvements regardless of the underlying model or provider.

<Columns cols={2}>
  <Card title="Automatic Response Caching" icon="zap">
    Zero-configuration caching for all non-streaming responses.
  </Card>
  <Card title="Automatic Cache Key Generation" icon="key">
    Intelligent cache key generation based on request content.
  </Card>
  <Card title="Configurable TTL" icon="clock">
    Flexible time-to-live settings with 60 second default.
  </Card>
  <Card title="Provider Agnostic" icon="shuffle">
    Works seamlessly across all 7 AI providers.
  </Card>
  <Card title="Performance Metrics" icon="bar-chart">
    Built-in cache hit/miss tracking and latency measurements.
  </Card>
  <Card title="Memory-Efficient Storage" icon="database">
    Optimised in-memory cache with automatic expiration.
  </Card>
</Columns>
