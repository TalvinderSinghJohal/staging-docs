---
title: Hallucinations
description: "Detailed overview of each AltrumAI policy including purpose, features, and usage guidance."
---

## Overview

The **Hallucinations Policy** is designed to help organisations manage the risk of inaccurate, fabricated, or misleading information in AI-generated content. This policy applies solely to **responses** produced by the Language Model (LLM), offering simple enforcement controls to monitor or block outputs that do not align with factual accuracy or verified information.
By activating this policy, organisations can reduce the risk of misinformation, preserve credibility, and ensure AI responses are trustworthy and aligned with organisational standards.

---

## What the Policy Does

### Purpose

The Hallucinations Policy safeguards against the generation of responses that may contain:

- Fabricated facts or data.
- Misleading or unverifiable claims.
- Inaccurate references or attributions.

By identifying and acting on such content, the policy supports responsible AI communication—especially in high-stakes environments like legal, medical, financial, and regulatory use cases.

### Scope

#### Response Configuration Only

This policy exclusively applies to **AI-generated responses**. It does not evaluate or restrict user-submitted prompts.

#### Operational Modes

- **Log Only**: Detects and logs hallucinated content without blocking the response.
- **Log and Override**: Blocks the LLM’s output if it contains hallucinated or unverifiable information.

### Key Features

- **Response-Only Monitoring**: Targets hallucinations in AI outputs.
- **Flexible Enforcement Controls**: Choose to monitor or block fabricated content.
- **Accuracy Safeguards**: Helps protect organisational trust, reliability, and compliance.
- **Streamlined Configuration**: Minimal setup for maximum impact in high-risk environments.

---

## Why Use This Policy?

### Benefits

- Reduces the spread of false or misleading information.
- Enhances user trust and confidence in AI-generated responses.
- Supports compliance in regulated industries.
- Provides oversight and transparency through detailed logging.

---

## Use Case: Compliance Support in Financial Services

### Scenario

A global financial institution uses an LLM to help employees interpret regulations, summarise guidelines, and generate internal policy content. Given the high risk of misinformation, accuracy is non-negotiable.

### Challenge

The organisation must:

- Detect and block LLM responses that present fabricated or unverifiable content.
- Maintain strict quality assurance and regulatory compliance.
- Log all hallucination incidents for continuous improvement and auditing.

### Solution: Implementing the Hallucinations Policy

1. **Response Filtering**  
   - Enabled to monitor and control all LLM-generated outputs.

2. **Enforcement Mode**  
   - Configured to **Log and Override** to prevent the delivery of hallucinated content.

---

## How to Use the Policy

> **Note:** The following steps explain how to configure the RAG Hallucinations Policy within the policy workflow. This policy applies **only to AI-generated responses** and cannot be applied to prompts.

### Step 1: Navigate to the Policy Workflow

1. From the **Dashboard**, select your project to access the **Project Overview**.
2. In the **Policy** section, click **Edit Policy** to open the policy configuration workflow.

---

### Step 2: Select and Enable the RAG Hallucinations Policy

1. In the **Configure Policies** tab, locate and click on **RAG Hallucinations** from the policy list.
2. The configuration panel will open on the right.
3. Toggle the **Enable Policy** switch to **ON** to activate the configuration options.

---

### Step 3: Configure Enforcement Behaviour

1. Under **Behaviour**, choose how the system should handle hallucinations:
   - **Log Only** – Record hallucinated responses for visibility and auditing.
   - **Log and Override** – Block hallucinated responses and return a smart fallback message to the user.

---

### Step 4: Save, Test, and Apply the Policy

1. Click **Save Changes** to store your configuration.
2. *(Optional)* Navigate to the **Test Policies** tab to simulate responses and validate how hallucinations are detected and managed.
3. Return to **Configure Policies** and click **Apply Policies** to enforce your settings.
4. A confirmation message will confirm that the policy has been successfully applied.

---

The **RAG Hallucinations Policy** ensures that AI-generated responses based on retrieval-augmented generation are accurate, grounded, and verifiable—minimising the risk of fabricated or misleading information reaching end users.

