---
title: Hallucinations
description: "Detailed overview of each AltrumAI policy including purpose, features, and usage guidance."
---

## Overview

The **Hallucinations Policy** is designed to help organisations manage the risk of inaccurate, fabricated, or misleading information in AI-generated content. This policy applies solely to **responses** produced by the Language Model (LLM), offering simple enforcement controls to monitor or block outputs that do not align with factual accuracy or verified information.
By activating this policy, organisations can reduce the risk of misinformation, preserve credibility, and ensure AI responses are trustworthy and aligned with organisational standards.

<CardGroup cols={2}>
  <Card
    title="Data Privacy"
    icon="lock"
    href="#data-privacy-policy"
  >
    Prevents the transmission or exposure of sensitive data in prompts and responses.
  </Card>
  <Card
    title="Bias & Fairness"
    icon="balance-scale"
    href="#bias-and-fairness-policy"
  >
    (Coming Soon) Ensure equitable AI behavior across demographics.
  </Card>
  <Card
    title="Toxic Content"
    icon="radiation"
    href="#toxic-content-policy"
  >
    (Coming Soon) Blocks harmful, offensive, or inappropriate outputs.
  </Card>
</CardGroup>

---

## What the Policy Does

### Purpose

The Hallucinations Policy safeguards against the generation of responses that may contain:

- Fabricated facts or data.
- Misleading or unverifiable claims.
- Inaccurate references or attributions.

By identifying and acting on such content, the policy supports responsible AI communication—especially in high-stakes environments like legal, medical, financial, and regulatory use cases.

### Scope

#### Response Configuration Only

This policy exclusively applies to **AI-generated responses**. It does not evaluate or restrict user-submitted prompts.

#### Operational Modes

- **Log Only**: Detects and logs hallucinated content without blocking the response.
- **Log and Override**: Blocks the LLM’s output if it contains hallucinated or unverifiable information.

### Key Features

- **Response-Only Monitoring**: Targets hallucinations in AI outputs.
- **Flexible Enforcement Controls**: Choose to monitor or block fabricated content.
- **Accuracy Safeguards**: Helps protect organisational trust, reliability, and compliance.
- **Streamlined Configuration**: Minimal setup for maximum impact in high-risk environments.

---

## Why Use This Policy?

### Benefits

- Reduces the spread of false or misleading information.
- Enhances user trust and confidence in AI-generated responses.
- Supports compliance in regulated industries.
- Provides oversight and transparency through detailed logging.

---

## Use Case: Compliance Support in Financial Services

### Scenario

A global financial institution uses an LLM to help employees interpret regulations, summarise guidelines, and generate internal policy content. Given the high risk of misinformation, accuracy is non-negotiable.

### Challenge

The organisation must:

- Detect and block LLM responses that present fabricated or unverifiable content.
- Maintain strict quality assurance and regulatory compliance.
- Log all hallucination incidents for continuous improvement and auditing.

### Solution: Implementing the Hallucinations Policy

1. **Response Filtering**  
   - Enabled to monitor and control all LLM-generated outputs.

2. **Enforcement Mode**  
   - Configured to **Log and Override** to prevent the delivery of hallucinated content.

---

## How to Use the Policy

> **Note:** This section will be updated with detailed UI setup guidance.

### Enabling the Policy

*Placeholder for enabling steps.*

### Setting Enforcement Mode

*Placeholder for selecting Log Only or Log and Override.*

### Saving and Applying the Policy

*Placeholder for finalising and activating policy settings.*

---

With the **Hallucinations Policy**, AltrumAI enables organisations to confidently deliver accurate, verifiable, and responsible AI-generated content—minimising risk and maximising trust.
