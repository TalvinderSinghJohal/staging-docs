---
title: Bias & Fairness
description: "Detailed overview of each AltrumAI policy including purpose, features, and usage guidance."
---

## Overview

The **Bias & Fairness Policy** helps organisations ensure that interactions with Language Models (LLMs) are inclusive, balanced, and free from unintended bias. By monitoring and managing biased content in both user inputs and AI-generated responses, this policy supports ethical and equitable AI usage across diverse environments.
Built with the same configuration simplicity as the Toxicity Policy, this policy focuses exclusively on the detection and control of bias and fairness issues—offering a lightweight but powerful layer of oversight for AI behaviour.

## What the Policy Does

### Purpose

The Bias & Fairness Policy is designed to identify and manage language that may exhibit or reinforce social, cultural, gender, or demographic biases. It helps prevent the AI from engaging in or perpetuating stereotypes, discriminatory perspectives, or imbalanced narratives.

By enabling this policy, organisations can uphold fairness in communication and foster responsible AI deployment.

### Scope

#### Prompt & Response Configuration

The policy can be applied to both:

- **Prompts**: User-submitted inputs that may include biased language.
- **Responses**: AI-generated outputs that could unintentionally reinforce bias.

Organisations can choose to enable filtering on one or both ends, depending on internal fairness standards and regulatory needs.

#### Operational Modes

- **Log Only**: Monitors and records bias-related content without restricting the flow.
- **Log and Override**: Blocks prompts or responses that are flagged for bias, ensuring users are not exposed to unfair or inappropriate content.

#### Threshold Sensitivity

The detection strictness is configurable with a threshold range of **0.2 to 0.9**:

- **Lower thresholds** (e.g., 0.2) allow broader detection and awareness.
- **Higher thresholds** (e.g., 0.9) enforce stricter filtering to prevent even subtle bias.

### Key Features

- **Bias Detection in Prompts and Responses**: Actively monitors both user input and AI output.
- **Customisable Sensitivity Threshold**: Set the detection strictness to align with organisational values.
- **Two Enforcement Modes**: Choose between passive monitoring and active blocking.
- **Focused, Lightweight Configuration**: Simple setup with high impact.

---

## Why Use This Policy?

### Benefits

- Promotes fairness and inclusivity in AI interactions.
- Helps prevent the spread of biased or discriminatory perspectives.
- Supports compliance with DEI, legal, and regulatory standards.
- Increases transparency and accountability across AI workflows.

---

## Use Case: Inclusive Customer Support AI

### Scenario

A national insurance company uses an AI assistant for customer service inquiries. As the assistant interacts with a diverse customer base, leadership wants to ensure it communicates in a neutral, fair, and respectful way, avoiding responses that could suggest cultural, gender, or age bias.

### Challenge

The organisation must:

- Detect potentially biased language in user prompts.
- Prevent the AI from returning responses that include harmful or unbalanced representations.
- Ensure alignment with corporate values on diversity and fairness.

### Solution: Implementing the Bias & Fairness Policy

1. **Prompt & Response Filtering**  
   - Enabled for both inputs and outputs to ensure full oversight.

2. **Enforcement Mode**  
   - Configured as **Log and Override** to block biased content entirely.

3. **Threshold Sensitivity**  
   - Set to **0.8** to ensure strict, meaningful detection without excessive false positives.

---

## How to Use the Policy

> **Note:** The steps below guide you through configuring the Bias & Fairness Policy in the policy workflow interface.

### Step 1: Navigate to the Policy Workflow

1. From the **Dashboard**, open your project to access the **Project Overview**.
2. Click **Edit Policy** in the Policy section to begin configuration.

![Screenshot: Access policy workflow](#)

---

### Step 2: Select and Enable the Bias & Fairness Policy

1. In the **Configure Policies** tab, click on **Bias & Fairness** from the list.
2. The configuration panel will display on the right.
3. Toggle **Enable Policy** to **ON** to begin editing.

![Screenshot: Enable Bias & Fairness policy](#)

---

### Step 3: Set Application Scope

1. Under **Apply Policy To**, choose one:
   - **Prompt**
   - **Response**
   - **Both**

This determines whether the policy is applied to user input, AI output, or both.

![Screenshot: Scope selection](#)

---

### Step 4: Configure Enforcement Behaviour

1. Choose your policy behaviour:
   - **Log Only** – Log detected bias without blocking.
   - **Log and Override** – Block biased content and return a smart replacement.

![Screenshot: Select Log Only or Log and Override](#)

---

### Step 5: Adjust Detection Threshold

1. Use the **Threshold Slider** to define the level of detection strictness:
   - **Lower values** detect a broader range of potential bias.
   - **Higher values** are stricter and more precise.

![Screenshot: Threshold control](#)

---

### Step 6: Save, Test, and Apply

1. Click **Save Changes** to store your configuration.
2. *(Optional)* Go to **Test Policies** to preview how the policy behaves in live chat.
3. Click **Apply Policies** in the **Configure Policies** tab to activate it.
4. A confirmation message will verify that the policy is now active.

![Screenshot: Save and apply](#)

---

The **Bias & Fairness Policy** ensures your AI interactions remain inclusive, balanced, and free from inappropriate bias—supporting ethical and equitable user experiences.
